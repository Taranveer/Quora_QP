{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "# shared global variables to be imported from model also\n",
    "UNK = \"$UNK$\"\n",
    "NUM = \"$NUM$\"\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, filename, emb_file):\n",
    "        self.filename = filename\n",
    "        self.emb_file = emb_file\n",
    "\n",
    "\n",
    "    def generate_vocab(self):\n",
    "        text_vocab = self.get_vocabs()\n",
    "        emb_vocab = self.get_w2v_vocab()\n",
    "        self.vocab = text_vocab & emb_vocab\n",
    "        # self.vocab.add(UNK)\n",
    "        return self.vocab\n",
    "\n",
    "\n",
    "\n",
    "    def get_vocabs(self):\n",
    "        \"\"\"\n",
    "        This function just returns the vocabulary in the text\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        print \"Building vocab from text ...\"\n",
    "        with open(self.filename) as f:\n",
    "            data = pickle.load(f)\n",
    "            data = data[0]\n",
    "            data = [x.split() for x in data]\n",
    "            vocab_words= set()\n",
    "            for line in data:\n",
    "                vocab_words.update(line)\n",
    "            print(\"- done. {} tokens\".format(len(vocab_words)))\n",
    "        return vocab_words\n",
    "\n",
    "    def get_w2v_vocab(self):\n",
    "        \"\"\"\n",
    "        This function return the vocabulary we have in the\n",
    "        word embedding file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        print \"Building vocab from embeddings ...\"\n",
    "        vocab = set()\n",
    "        with open(self.emb_file) as f:\n",
    "            for line in f:\n",
    "                word = line.strip().split(' ')[0]\n",
    "                vocab.add(word)\n",
    "        print(\"- done. {} tokens\".format(len(vocab)))\n",
    "        return vocab\n",
    "\n",
    "def write_vocab(vocab, filename):\n",
    "    \"\"\"\n",
    "    Writes a vocab to a file\n",
    "    :param vocab:\n",
    "    :param filename:\n",
    "    :return: write a word per line\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Writing vocab...\")\n",
    "    with open(filename, \"w\") as f:\n",
    "        for i, word in enumerate(vocab):\n",
    "            if i != len(vocab) - 1:\n",
    "                f.write(\"{}\\n\".format(word))\n",
    "            else:\n",
    "                f.write(word)\n",
    "    print(\"- done. {} tokens\".format(len(vocab)))\n",
    "\n",
    "def load_vocab(filename):\n",
    "    \"\"\"\n",
    "    Loads vocab from a file\n",
    "    :param filename:\n",
    "    :return: d: dict[word] = index\n",
    "    \"\"\"\n",
    "    try:\n",
    "        d = dict()\n",
    "        with open(filename) as f:\n",
    "            for idx, word in enumerate(f):\n",
    "                word = word.strip()\n",
    "                d[word] = idx\n",
    "\n",
    "    except IOError:\n",
    "        print \"Generate the vocabulary and embeddings first\"\n",
    "    return d\n",
    "\n",
    "\n",
    "def export_trimmed_w2v_vectors(vocab, glove_filename, trimmed_filename, dim=300):\n",
    "    \"\"\"Saves glove vectors in numpy array\n",
    "\n",
    "    Args:\n",
    "        vocab: dictionary vocab[word] = index\n",
    "        glove_filename: a path to a glove file\n",
    "        trimmed_filename: a path where to store a matrix in npy\n",
    "        dim: (int) dimension of embeddings\n",
    "\n",
    "    \"\"\"\n",
    "    embeddings = np.zeros([len(vocab), dim])\n",
    "    with open(glove_filename) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(' ')\n",
    "            word = line[0]\n",
    "            embedding = [float(x) for x in line[1:]]\n",
    "            if word in vocab:\n",
    "                word_idx = vocab[word]\n",
    "                embeddings[word_idx] = np.asarray(embedding)\n",
    "\n",
    "    np.savez_compressed(trimmed_filename, embeddings=embeddings)\n",
    "\n",
    "\n",
    "def get_trimmed_w2v_vectors(filename):\n",
    "    \"\"\"\n",
    "    Load embeddings vectors\n",
    "    :param filename:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with np.load(filename) as data:\n",
    "            return data[\"embeddings\"]\n",
    "\n",
    "    except IOError:\n",
    "        print \"File: %s, NOT FOUND.\"%(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab from text ...\n",
      "- done. 17443 tokens\n",
      "Building vocab from embeddings ...\n",
      "- done. 2196016 tokens\n",
      "Writing vocab...\n",
      "- done. 16836 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16836, 300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"../\"\n",
    "dataset = Dataset(data_path + \"corpus/sentences_train_10000.pkl\", \"../glove.840B.300d.txt\")\n",
    "dataset.generate_vocab()\n",
    "write_vocab(dataset.vocab, data_path + \"vocab_embedding/vocab_quora_train.txt\")\n",
    "vocab = load_vocab(data_path + \"vocab_embedding/vocab_quora_train.txt\")\n",
    "export_trimmed_w2v_vectors(vocab, data_path + \"../glove.840B.300d.txt\", data_path + \"vocab_embedding/trimmed_embeddings_train_quora.npz\")\n",
    "glove_array = get_trimmed_w2v_vectors(data_path + \"vocab_embedding/trimmed_embeddings_train_quora.npz\")\n",
    "glove_array.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
