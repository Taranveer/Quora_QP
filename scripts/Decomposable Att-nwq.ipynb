{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dynet as dy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = pickle.load(open(\"../corpus/labels_40000.pkl\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_model_filename = \"../models/sentence_structure_40k.npz\"\n",
    "def load_model():\n",
    "    with np.load(sentence_model_filename) as data:\n",
    "        sent_indices = data[\"sent_indices\"]\n",
    "        sent_mask = data[\"sent_mask\"]\n",
    "    return sent_indices, sent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(808580, 245)\n"
     ]
    }
   ],
   "source": [
    "sent_indices, sent_mask = load_model()\n",
    "sent_count_old = sent_indices.shape[0]\n",
    "max_length = sent_indices.shape[1]\n",
    "print(sent_count_old, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty_index1 = np.argwhere(np.sum(sent_mask[sent_count_old//2:sent_count_old,:], axis = 1, dtype = int)== 0)\n",
    "empty_index2 = np.argwhere(np.sum(sent_mask[:sent_count_old//2,:], axis = 1, dtype = int) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(808580, 808398)\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "l = np.union1d(empty_index1,empty_index2)\n",
    "empty_count = l.shape[0]\n",
    "sent_count = sent_count_old - 2*empty_count\n",
    "print(sent_count_old,sent_count)\n",
    "print(empty_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_ind = np.zeros((sent_count,max_length))\n",
    "sent_masks = np.zeros((sent_count,max_length))\n",
    "sent_ind[0:sent_count//2,:] = np.delete(sent_indices[:sent_count_old//2], l, axis = 0)\n",
    "sent_ind[sent_count//2:sent_count,:] = np.delete(sent_indices[sent_count_old//2:sent_count_old], l, axis = 0)\n",
    "sent_masks[0:sent_count//2,:] = np.delete(sent_mask[:sent_count_old//2], l, axis = 0)\n",
    "sent_masks[sent_count//2:sent_count,:] = np.delete(sent_mask[sent_count_old//2:sent_count_old], l, axis = 0)\n",
    "sent_masks = np.sum(sent_masks,axis=1)\n",
    "sent_masks = sent_masks.astype(int)\n",
    "labels = np.delete(labels, l, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(808398, 245)\n",
      "(808398,)\n",
      "(404199,)\n"
     ]
    }
   ],
   "source": [
    "print(sent_ind.shape)\n",
    "print(sent_masks.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(485040, 245)\n",
      "(485040,)\n",
      "(242520, 245)\n",
      "(242520,)\n",
      "(80838, 245)\n",
      "(80838,)\n"
     ]
    }
   ],
   "source": [
    "def create_split(sent_ind,sent_masks,labels):\n",
    "    #242520 train; 121260 valid; 40419 test\n",
    "    trc = 242520\n",
    "    vc = 121260\n",
    "    tec = 40419\n",
    "    train_ind = np.vstack((sent_ind[:trc,:],sent_ind[sent_count//2:sent_count//2+trc,:]))\n",
    "    train_masks = np.hstack((sent_masks[:trc],sent_masks[sent_count//2:sent_count//2+trc]))\n",
    "    valid_ind = np.vstack((sent_ind[trc:trc+vc,:],sent_ind[sent_count//2+trc:sent_count//2+trc+vc,:]))\n",
    "    valid_masks = np.hstack((sent_masks[trc:trc+vc],sent_masks[sent_count//2+trc:sent_count//2+trc+vc]))\n",
    "    test_ind = np.vstack((sent_ind[trc+vc:trc+vc+tec,:],sent_ind[sent_count//2+trc+vc:sent_count//2+trc+vc+tec,:]))\n",
    "    test_masks = np.hstack((sent_masks[trc+vc:trc+vc+tec],sent_masks[sent_count//2+trc+vc:sent_count//2+trc+vc+tec]))\n",
    "    return train_ind,train_masks,valid_ind,valid_masks,test_ind,test_masks\n",
    "\n",
    "train_ind,train_masks,valid_ind,valid_masks,test_ind,test_masks = create_split(sent_ind,sent_masks,labels)\n",
    "print(train_ind.shape)\n",
    "print(train_masks.shape)\n",
    "print(valid_ind.shape)\n",
    "print(valid_masks.shape)\n",
    "print(test_ind.shape)\n",
    "print(test_masks.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposable Attention Part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss after 1 epochs: 0.492156668986\n",
      "Train acc after 1 epochs: 0.76761916543\n",
      "Validation loss after 1 epochs: 0.528400424684\n",
      "Validation acc after 1 epochs: 0.749843311892\n",
      "Train loss after 2 epochs: 0.467627568869\n",
      "Train acc after 2 epochs: 0.799612403101\n",
      "Validation loss after 2 epochs: 0.579011909164\n",
      "Validation acc after 2 epochs: 0.759896091044\n",
      "Train loss after 3 epochs: 0.466117348183\n",
      "Train acc after 3 epochs: 0.823684644565\n",
      "Validation loss after 3 epochs: 0.691217346661\n",
      "Validation acc after 3 epochs: 0.761050634999\n",
      "Train loss after 4 epochs: 0.433422922588\n",
      "Train acc after 4 epochs: 0.844750948375\n",
      "Validation loss after 4 epochs: 0.750514796467\n",
      "Validation acc after 4 epochs: 0.762757710704\n",
      "Train loss after 5 epochs: 0.430042815203\n",
      "Train acc after 5 epochs: 0.857809665182\n",
      "Validation loss after 5 epochs: 0.864245364185\n",
      "Validation acc after 5 epochs: 0.760465116279\n",
      "Train loss after 6 epochs: 0.424883240841\n",
      "Train acc after 6 epochs: 0.867483094178\n",
      "Validation loss after 6 epochs: 0.973479810362\n",
      "Validation acc after 6 epochs: 0.757694210787\n",
      "Train loss after 7 epochs: 0.408571611574\n",
      "Train acc after 7 epochs: 0.878467755237\n",
      "Validation loss after 7 epochs: 1.09019139049\n",
      "Validation acc after 7 epochs: 0.75669635494\n",
      "Train loss after 8 epochs: 0.410344480207\n",
      "Train acc after 8 epochs: 0.885126999835\n",
      "Validation loss after 8 epochs: 1.22657497342\n",
      "Validation acc after 8 epochs: 0.754346033317\n",
      "Train loss after 9 epochs: 0.413406748647\n",
      "Train acc after 9 epochs: 0.890145142669\n",
      "Validation loss after 9 epochs: 1.36651904972\n",
      "Validation acc after 9 epochs: 0.752647204354\n"
     ]
    }
   ],
   "source": [
    "#def similarity():\n",
    "    \n",
    "#data is already in data numpy matrix\n",
    "\n",
    "class decomposable_attention():\n",
    "    def __init__(self,sent_indices, sent_mask, labels, embedding_dim, debug=False):\n",
    "        trc = 242520\n",
    "        vc = 121260\n",
    "        tec = 40419\n",
    "        \n",
    "        self.pc = dy.Model()\n",
    "        self.debug = debug\n",
    "        self.embedding_dim = embedding_dim \n",
    "        self.embedding_matrix = self.pc.add_lookup_parameters((16836,embedding_dim))\n",
    "        \n",
    "        train,train_masks,valid,valid_masks,test,test_masks = create_split(sent_indices,sent_mask,labels)\n",
    "        \n",
    "        self.tr_labels = labels[:trc]\n",
    "        self.v_labels = labels[trc:trc+vc]\n",
    "        self.te_labels = labels[trc+vc:]\n",
    "        \n",
    "        self.train1 = train[:trc,:]\n",
    "        self.train2 = train[trc:2*trc,:]\n",
    "        \n",
    "        self.valid1 = valid[:vc,:]\n",
    "        self.valid2 = valid[vc:2*vc,:]\n",
    "        \n",
    "        self.test1 = test[:tec,:]\n",
    "        self.test2 = test[tec:2*tec,:]\n",
    "        \n",
    "        self.tr_mask1 = train_masks[:trc]\n",
    "        self.tr_mask2 = train_masks[trc:2*trc]\n",
    "        \n",
    "        self.v_mask1 = valid_masks[:vc]\n",
    "        self.v_mask2 = valid_masks[vc:2*vc]\n",
    "        \n",
    "        self.te_mask1 = test_masks[:tec]\n",
    "        self.te_mask2 = test_masks[tec:2*tec]\n",
    "        \n",
    "        self.w2 = self.pc.add_parameters((embedding_dim,2*embedding_dim))\n",
    "        self.b2 = self.pc.add_parameters((embedding_dim,1))\n",
    "        \n",
    "        self.w3 = self.pc.add_parameters((embedding_dim,2*embedding_dim))\n",
    "        self.b3 = self.pc.add_parameters((embedding_dim,1))\n",
    "        \n",
    "        self.w = self.pc.add_parameters((1,2*embedding_dim))\n",
    "        self.b = self.pc.add_parameters((1,1))\n",
    "        \n",
    "        \n",
    "    def forward(self,x1,x2,label,k,mode):\n",
    "        \n",
    "        debug = self.debug\n",
    "        \n",
    "        if mode=='Train':\n",
    "            len1 = self.tr_mask1\n",
    "            len2 = self.tr_mask2\n",
    "        elif mode=='Valid':\n",
    "            len1 = self.v_mask1\n",
    "            len2 = self.v_mask2\n",
    "        else:\n",
    "            len1 = self.te_mask1\n",
    "            len2 = self.te_mask2\n",
    "            \n",
    "        embedding_dim = self.embedding_dim\n",
    "        \n",
    "        w = dy.parameter(self.w)\n",
    "        b = dy.parameter(self.b)\n",
    "        w2 = dy.parameter(self.w2)\n",
    "        b2 = dy.parameter(self.b2)\n",
    "        w3 = dy.parameter(self.w3)\n",
    "        b3 = dy.parameter(self.b3)\n",
    "        \n",
    "        embeds1 = dy.reshape(dy.lookup_batch(self.embedding_matrix,x1),(len1[k],embedding_dim))\n",
    "        embeds2 = dy.reshape(dy.lookup_batch(self.embedding_matrix,x2),(len2[k],embedding_dim))\n",
    "\n",
    "        if debug:\n",
    "            print('embedding 1:', (len1[k],embedding_dim), embeds1.dim())\n",
    "            print('embedding 2:', (embedding_dim,len2[k]), embeds2.dim())\n",
    "\n",
    "        similarity = embeds1*dy.transpose(embeds2)\n",
    "        if debug:\n",
    "            print('similarity dimension:', (len1[k],len2[k]), similarity.dim())\n",
    "\n",
    "        n_a = dy.softmax(similarity,d=0)\n",
    "        n_b = dy.softmax(similarity,d=1)\n",
    "        \n",
    "        alpha = dy.transpose(n_a)*embeds1\n",
    "        beta = n_b*embeds2\n",
    "        \n",
    "        if debug:\n",
    "            print('alpha:',(len2[k],embedding_dim), alpha.dim())\n",
    "            print('beta:',(len1[k],embedding_dim), beta.dim())\n",
    "        \n",
    "        #print((w2*dy.transpose(dy.concatenate_cols([embeds1,beta]))).npvalue().shape)\n",
    "        #print(b2.npvalue().shape)\n",
    "        \n",
    "        v1i = w2*dy.transpose(dy.concatenate_cols([embeds1,beta])) + b2\n",
    "        v2j = w3*dy.transpose(dy.concatenate_cols([embeds2,alpha])) + b3\n",
    "        if debug:\n",
    "            print('v1:', (embedding_dim,len1[k]), v1i.dim())\n",
    "            print('v2:', (embedding_dim,len2[k]), v2j.dim())\n",
    "        \n",
    "        v1 = dy.mean_dim(v1i,[1],0)\n",
    "        v1 = dy.reshape(v1,(embedding_dim,1)) \n",
    "        \n",
    "        v2 = dy.mean_dim(v2j,[1],0)\n",
    "        v2 = dy.reshape(v2,(embedding_dim,1)) \n",
    "        \n",
    "        score = w*dy.concatenate([v1,v2],d=0) + b\n",
    "        \n",
    "        return score\n",
    "        \n",
    "    def train(self):\n",
    "        dev_iter = 100000\n",
    "        embedding_dim = self.embedding_dim\n",
    "        lr = 0.0003\n",
    "        trainer = dy.AdamTrainer(self.pc, alpha = lr)\n",
    "        \n",
    "        itr = 0\n",
    "        for epochs in range(15):\n",
    "            tl = 0\n",
    "            for k in range(len(self.tr_labels)):\n",
    "                itr += 1\n",
    "                dy.renew_cg()\n",
    "                x1 = self.train1[k,0:self.tr_mask1[k]]\n",
    "                x2 = self.train2[k,0:self.tr_mask2[k]]\n",
    "                label = self.tr_labels[k]\n",
    "\n",
    "                score = self.forward(x1,x2,label,k,mode='Train')\n",
    "                norm_score = dy.logistic(score)\n",
    "                \n",
    "                loss = dy.binary_log_loss(norm_score,dy.inputTensor([[label]]))\n",
    "                loss.backward()\n",
    "                trainer.update()\n",
    "                tl += loss.scalar_value()\n",
    "                #print valid scores every dev_iter iterations\n",
    "#                 if itr % dev_iter == 0:\n",
    "#                     self.predict('Valid', itr)\n",
    "                    \n",
    "            if epochs in [2,5,8,10] :\n",
    "                lr /= 2\n",
    "                trainer = dy.AdamTrainer(self.pc, alpha = lr)\n",
    "            #print train and valid scores at the end of every epoch        \n",
    "            self.predict('Train', 1+epochs, True)\n",
    "            self.predict('Valid', 1+epochs, True)\n",
    "        #print final scores    \n",
    "        #self.predict('Train',1+epochs,True) already calculated\n",
    "        #self.predict('Valid',1+epochs,True)\n",
    "        self.predict('Test',1+epochs,True)\n",
    "            \n",
    "    def predict(self, mode, count, is_epoch_end=False):\n",
    "        if is_epoch_end:\n",
    "            if mode=='Valid':\n",
    "                vl = 0\n",
    "                preds_v = []\n",
    "                for k in range(len(self.v_labels)):\n",
    "                    dy.renew_cg()\n",
    "                    x1 = self.valid1[k,0:self.v_mask1[k]]\n",
    "                    x2 = self.valid2[k,0:self.v_mask2[k]]\n",
    "                    label = self.v_labels[k]\n",
    "\n",
    "                    score = self.forward(x1,x2,label,k,mode='Valid')\n",
    "                    norm_score = dy.logistic(score)\n",
    "                    preds_v.append(norm_score.scalar_value()>0.5)\n",
    "                    loss = dy.binary_log_loss(norm_score,dy.inputTensor([[label]]))\n",
    "                    \n",
    "                    vl += loss.scalar_value()\n",
    "                v_acc = sum(1 for x,y in zip(self.v_labels,preds_v) if x == y) / len(self.v_labels)\n",
    "                print('Validation loss after ' + str(count) + ' epochs: ' + str(vl/len(self.v_labels)))\n",
    "                print('Validation acc after ' + str(count) + ' epochs: ' + str(v_acc))\n",
    "\n",
    "            elif mode=='Test':\n",
    "                tel = 0\n",
    "                preds_te = []\n",
    "                for k in range(len(self.te_labels)):\n",
    "                    dy.renew_cg()\n",
    "                    x1 = self.test1[k,0:self.te_mask1[k]]\n",
    "                    x2 = self.test2[k,0:self.te_mask2[k]]\n",
    "                    label = self.te_labels[k]\n",
    "\n",
    "                    score = self.forward(x1,x2,label,k,mode='Test')\n",
    "                    norm_score = dy.logistic(score)\n",
    "                    preds_te.append(norm_score.scalar_value()>0.5)\n",
    "                    loss = dy.binary_log_loss(norm_score,dy.inputTensor([[label]]))\n",
    "\n",
    "                    tel += loss.scalar_value()\n",
    "                te_acc = sum(1 for x,y in zip(self.te_labels,preds_te) if x == y) / len(self.te_labels)\n",
    "                print('Test loss after ' + str(count) + ' epochs: ' + str(tel/len(self.te_labels)))\n",
    "                print('Test acc after ' + str(count) + ' epochs: ' + str(te_acc))\n",
    "\n",
    "            else:\n",
    "                trl = 0\n",
    "                preds_tr = []\n",
    "                for k in range(len(self.tr_labels)):\n",
    "                    dy.renew_cg()\n",
    "                    x1 = self.train1[k,0:self.tr_mask1[k]]\n",
    "                    x2 = self.train2[k,0:self.tr_mask2[k]]\n",
    "                    label = self.tr_labels[k]\n",
    "\n",
    "                    score = self.forward(x1,x2,label,k,mode='Train')\n",
    "                    norm_score = dy.logistic(score)\n",
    "                    a = norm_score.scalar_value()>0.5\n",
    "                    preds_tr.append(norm_score.scalar_value()>0.5)\n",
    "                    loss = dy.binary_log_loss(norm_score,dy.inputTensor([[label]]))\n",
    "\n",
    "                    trl += loss.scalar_value()\n",
    "                tr_acc = sum(1 for x,y in zip(self.tr_labels,preds_tr) if x == y) / len(self.tr_labels)\n",
    "                print('Train loss after ' + str(count) + ' epochs: ' + str(trl/len(self.tr_labels)))\n",
    "                print('Train acc after ' + str(count) + ' epochs: ' + str(tr_acc))\n",
    "\n",
    "        else:\n",
    "            vl = 0\n",
    "            preds_v = []\n",
    "            for k in range(len(self.v_labels)):\n",
    "                dy.renew_cg()\n",
    "                x1 = self.valid1[k,0:self.v_mask1[k]]\n",
    "                x2 = self.valid2[k,0:self.v_mask2[k]]\n",
    "                label = self.v_labels[k]\n",
    "\n",
    "                score = self.forward(x1,x2,label,k,mode='Valid')\n",
    "                norm_score = dy.logistic(score)\n",
    "                preds_v.append(norm_score.scalar_value()>0.5)\n",
    "                loss = dy.binary_log_loss(norm_score,dy.inputTensor([[label]]))\n",
    "\n",
    "                vl += loss.scalar_value()\n",
    "            v_acc = sum(1 for x,y in zip(self.v_labels,preds_v) if x == y) / len(self.v_labels)\n",
    "            print('Validation loss after ' + str(count) + ' iterations: ' + str(vl/len(self.v_labels)))\n",
    "            print('Validation acc after ' + str(count) + ' iterations: ' + str(v_acc))\n",
    "\n",
    "def main():\n",
    "    \n",
    "    model = decomposable_attention(sent_ind, sent_masks, labels, embedding_dim=128, debug=False)\n",
    "    model.train()\n",
    "    \n",
    "main()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatched input dimensions in MatrixMultiply: [{222X10} {222X15}]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-165fff488ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_dynet.pyx\u001b[0m in \u001b[0;36m_dynet.Expression.__mul__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_dynet.pyx\u001b[0m in \u001b[0;36m_dynet._mul\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatched input dimensions in MatrixMultiply: [{222X10} {222X15}]"
     ]
    }
   ],
   "source": [
    "x1 = np.random.randint(5,size=10)\n",
    "x2 = np.random.randint(5,size=15)\n",
    "\n",
    "pc = dy.Model()\n",
    "embedding_matrix = pc.add_lookup_parameters((168000,222))\n",
    "a = dy.lookup_batch(embedding_matrix,x1)\n",
    "b = dy.lookup_batch(embedding_matrix,x2)\n",
    "\n",
    "c = a*b\n",
    "print(c.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.48167747, -0.07805443, -0.22833993,  0.49472263, -0.24113466,\n",
       "        0.788912  ,  0.25677112, -0.74309444])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pc = dy.Model()\n",
    "s = pc.add_parameters((4))\n",
    "s1 = pc.add_parameters((4))\n",
    "\n",
    "dy.renew_cg()\n",
    "sa = dy.parameter(s)\n",
    "sb = dy.parameter(s1)\n",
    "na = dy.concatenate([sa,sb])\n",
    "na.npvalue()\n",
    "#n_b = dy.softmax(similarity,d=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(document, W_cnn, b_cnn, W, b):\n",
    "        dy.renew_cg()\n",
    "        W_cnn = dy.parameter(W_cnn)\n",
    "        b_cnn = dy.parameter(b_cnn)\n",
    "        W = dy.parameter(W)\n",
    "        b = dy.parameter(b)\n",
    "\n",
    "\n",
    "        cnn_in = dy.concatenate([dy.lookup(lookup,x) for x in document], d=1)\n",
    "        cnn_out = dy.conv2d_bias(cnn_in, W_cnn, b_cnn, stride=(1, 2), is_valid=False)\n",
    "        cnn_out = dy.conv2d_bias(cnn_out, W, b, stride = (1,2), is_valid=False)\n",
    "        pool_out = dy.mean_dim(cnn_out, d=[1], b=0)\n",
    "        print(pool_out.npvalue().shape)\n",
    "        pool_out = dy.reshape(pool_out, (32,))\n",
    "        pool_out = dy.rectify(pool_out)\n",
    "\n",
    "        return pool_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n",
      "(1, 32)\n"
     ]
    }
   ],
   "source": [
    "len1 = np.sum(sent_mask[:10000,:], axis = 1, dtype = int)\n",
    "len2 = np.sum(sent_mask[10000:20000,:], axis = 1, dtype = int)\n",
    "\n",
    "pc = dy.Model()\n",
    "W_cnn = pc.add_parameters((1, 2, 128, 64)) # cnn weights\n",
    "b_cnn = pc.add_parameters((64))\n",
    "W = pc.add_parameters((1, 2, 64, 32)) # cnn weights\n",
    "b = pc.add_parameters((32))\n",
    "lookup = pc.add_lookup_parameters((20000, 1, 1, 128))\n",
    "for k in range(100):\n",
    "    document = sent_indices[k][0:len1[k]]\n",
    "    z = encoder(document, W_cnn, b_cnn, W, b)\n",
    "#     print(z.npvalue().shape)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((2,3))\n",
    "a = a.astype(int)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
