{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parse_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2c83c0e09a40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parse_args' is not defined"
     ]
    }
   ],
   "source": [
    "def parse_args():\n",
    "    parser = ArgumentParser(description= \"SIF Embedding\")\n",
    "\n",
    "    # data paths\n",
    "    #data_path = \"../corpus-tagged/book/\"\n",
    "    data_path = \"\"\n",
    "    parser.add_argument('--data_path', default=data_path, type = str)\n",
    "    parser.add_argument('--sentence_file', default=\"sentences.pkl\",type=str)\n",
    "    parser.add_argument('--embeddings_file', default='../glove.840B.300d.txt',type=str)\n",
    "\n",
    "\n",
    "    #save file path\n",
    "    parser.add_argument('--vocab_file', default=data_path+\"vocab_quora.txt\",type=str)\n",
    "    parser.add_argument('--trimmed_embedding_file', default=data_path+\"trimmed_embeddings_quora.npz\", type=str)\n",
    "\n",
    "    parser.add_argument('--object_file', default=data_path+\"object_file_quora.npz\", type=str)\n",
    "    #parser.add_argument('--concept_filenames', default=\"../concepts/concepts-union-clean.txt\", type=str)\n",
    "\n",
    "    # vocab building\n",
    "    parser.add_argument('--build', dest='build_vocab', action='store_true')\n",
    "    parser.set_defaults(build_vocab=False)\n",
    "\n",
    "    # remove pca component\n",
    "    parser.add_argument('--no_PCA', dest='pca', action='store_false')\n",
    "    parser.set_defaults(pca=True)\n",
    "\n",
    "    parser.add_argument('--n_components', default=1, type=int)\n",
    "\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SIF_Model(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.alpha = 1e-3\n",
    "        self.data = self.getData()\n",
    "        self.vocab = args.vocab\n",
    "        self.word_embeddings = args.word_embeddings\n",
    "        self.VOCAB_SIZE = len(self.vocab)\n",
    "        self.vocab_count = self.load_word_counters()\n",
    "        #self.getConceptVectors()\n",
    "        # self.loadModel()\n",
    "\n",
    "    def train(self):\n",
    "        self.weights = self.getWeightedProbabilities()\n",
    "        self.sent_indices, self.sent_mask = self.createStructure()\n",
    "        self.sent_weights = self.seq2weight(self.sent_indices, self.sent_mask, self.weights)\n",
    "        print self.sent_weights.shape\n",
    "        # self.saveEntries()\n",
    "        self.trainEmbeddings = self.SIF_embedding(self.word_embeddings, self.sent_indices, self.sent_weights)\n",
    "        print \"Model Training Completed. Start Saving\"\n",
    "\n",
    "    # def saveEntries(self):\n",
    "    #     np.savez_compressed(self.args.object_file, weights = self.weights,\n",
    "    #                         sent_weights = self.sent_weights, sent_indices = self.sent_indices)\n",
    "    #     print \"done\"\n",
    "    #\n",
    "    # def loadEntries(self):\n",
    "    #     with np.load(self.args.object_file) as data:\n",
    "    #         self.weights = data['weights']\n",
    "    #         self.sent_indices = data['sent_indices']\n",
    "    #         self.sent_weights = data['sent_weights']\n",
    "    #     print \"Entries Loaded\"\n",
    "\n",
    "    # def loadAndTrain(self):\n",
    "    #     self.loadEntries()\n",
    "    #     print \"Begin Training\"\n",
    "    #     self.trainEmbeddings = self.SIF_embedding(self.word_embeddings, self.sent_indices, self.sent_weights)\n",
    "\n",
    "    def saveModel(self):\n",
    "        np.savez_compressed(self.args.object_file, weights = self.weights, pca_components = self.pc, sif_embeddings = self.trainEmbeddings,\n",
    "                            )\n",
    "        print \"Model Saved\"\n",
    "\n",
    "\n",
    "    def loadModel(self):\n",
    "        with np.load(self.args.object_file) as data:\n",
    "            self.weights = data['weights']\n",
    "            self.pc = data['pca_components']\n",
    "            self.trainEmbeddings = data['sif_embeddings']\n",
    "        print \"Model Loaded\"\n",
    "\n",
    "\n",
    "    def getData(self):\n",
    "        with open(self.args.sentence_file) as f:\n",
    "            data = pickle.load(f)\n",
    "            data = data[0]\n",
    "            data = [x.split() for x in data]\n",
    "            return data\n",
    "\n",
    "    def load_word_counters(self):\n",
    "        vocab_count = Counter()\n",
    "        for line in self.data:\n",
    "            vocab_count.update(line)\n",
    "\n",
    "        # Filter Count\n",
    "        filtered_dict = {k:v for k,v in vocab_count.iteritems() if k in self.vocab}\n",
    "        return filtered_dict\n",
    "\n",
    "    def getWeightedProbabilities(self):\n",
    "        freqs = np.zeros((1, self.VOCAB_SIZE), dtype=\"float\")\n",
    "        for word in self.vocab:\n",
    "            idx = self.vocab[word]\n",
    "            freqs[0, idx] = self.vocab_count[word]\n",
    "\n",
    "        probs = freqs / np.sum(freqs)\n",
    "        weights = self.alpha / (self.alpha + probs)\n",
    "        return weights\n",
    "\n",
    "    def printShapes(self):\n",
    "        print \"Vocab Size: %s\"%(self.VOCAB_SIZE)\n",
    "        print self.weights.shape\n",
    "        print self.sent_indices.shape\n",
    "        print self.sent_indices.shape\n",
    "\n",
    "    def prepare_data(self,list_of_seqs):\n",
    "        lengths = [len(s) for s in list_of_seqs]\n",
    "        n_samples = len(list_of_seqs)\n",
    "        maxlen = np.max(lengths)\n",
    "        print maxlen\n",
    "        x = np.zeros((n_samples, maxlen)).astype('int32')\n",
    "        x_mask = np.zeros((n_samples, maxlen)).astype('float32')\n",
    "        for idx, s in enumerate(list_of_seqs):\n",
    "            x[idx, :lengths[idx]] = s\n",
    "            x_mask[idx, :lengths[idx]] = 1.\n",
    "        x_mask = np.asarray(x_mask, dtype='float32')\n",
    "        return x, x_mask\n",
    "\n",
    "#     def getConceptVectors(self):\n",
    "#         concepts = []\n",
    "#         with open(self.args.concept_filenames) as f:\n",
    "#             data  = f.read()\n",
    "#             data = data.split('\\n')\n",
    "#             for line in data:\n",
    "#                 concepts.append('t_'+\"_\".join(line.split())+'_t')\n",
    "\n",
    "#         concepts = list(set(concepts))\n",
    "#         self.concepts = filter(lambda x : x in self.vocab, concepts)\n",
    "#         concepts_indices = filter(lambda x : x is not None, map(self.vocab.get, self.concepts))\n",
    "#         print len(concepts_indices)\n",
    "#         self.concepts_embeddings = self.word_embeddings[concepts_indices]\n",
    "#         self.concepts_embeddings = normalize(self.concepts_embeddings)\n",
    "\n",
    "    def createStructure(self):\n",
    "        sentence_indices = []\n",
    "        sentence_weights = []\n",
    "        for line in self.data:\n",
    "            indices_list = filter(lambda x : x is not None, map(self.vocab.get, line))\n",
    "            # weight_list = []\n",
    "            # for idx in indices_list:\n",
    "            #     weight_list.append(self.weights[0,idx])\n",
    "            sentence_indices.append(indices_list)\n",
    "            # sentence_weights.append(weight_list)\n",
    "        x1, m1 = self.prepare_data(sentence_indices)\n",
    "        return x1, m1\n",
    "        # sentence_weights = np.asarray(sentence_weights, dtype='float')\n",
    "        # sentence_indices = np.asarray(sentence_indices, dtype='int')\n",
    "        # return sentence_indices, sentence_weights\n",
    "\n",
    "    def seq2weight(self, seq, mask, weight4ind):\n",
    "        weight = np.zeros(seq.shape).astype('float32')\n",
    "        for i in xrange(seq.shape[0]):\n",
    "            for j in xrange(seq.shape[1]):\n",
    "                if mask[i, j] > 0 and seq[i, j] >= 0:\n",
    "                    weight[i, j] = weight4ind[0, seq[i, j]]\n",
    "        weight = np.asarray(weight, dtype='float32')\n",
    "        return weight\n",
    "\n",
    "    def get_weighted_average(self, We, x, w):\n",
    "        \"\"\"\n",
    "        Compute the weighted average vectors\n",
    "        :param We: We[i,:] is the vector for word i\n",
    "        :param x: x[i, :] are the indices of the words in sentence i\n",
    "        :param w: w[i, :] are the weights for the words in sentence i\n",
    "        :return: emb[i, :] are the weighted average vector for sentence i\n",
    "        \"\"\"\n",
    "        n_samples = x.shape[0]\n",
    "        emb = np.zeros((n_samples, We.shape[1]))\n",
    "        for i in xrange(n_samples):\n",
    "            emb[i, :] = w[i, :].dot(We[x[i, :], :]) / (np.count_nonzero(w[i, :]) + 1.0)\n",
    "        return emb\n",
    "\n",
    "    def compute_pc(self,X, npc=1):\n",
    "        \"\"\"\n",
    "        Compute the principal components. DO NOT MAKE THE DATA ZERO MEAN!\n",
    "        :param X: X[i,:] is a data point\n",
    "        :param npc: number of principal components to remove\n",
    "        :return: component_[i,:] is the i-th pc\n",
    "        \"\"\"\n",
    "        svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n",
    "        svd.fit(X)\n",
    "        return svd.components_\n",
    "\n",
    "    def remove_pc(self, X, npc=1):\n",
    "        \"\"\"\n",
    "        Remove the projection on the principal components\n",
    "        :param X: X[i,:] is a data point\n",
    "        :param npc: number of principal components to remove\n",
    "        :return: XX[i, :] is the data point after removing its projection\n",
    "        \"\"\"\n",
    "        pc = self.compute_pc(X, npc)\n",
    "        self.pc = pc\n",
    "        if npc == 1:\n",
    "            XX = X - X.dot(pc.transpose()) * pc\n",
    "        else:\n",
    "            XX = X - X.dot(pc.transpose()).dot(pc)\n",
    "        return XX\n",
    "\n",
    "    def SIF_embedding(self, We, x, w):\n",
    "        \"\"\"\n",
    "        Compute the scores between pairs of sentences using weighted average + removing the projection on the first principal component\n",
    "        :param We: We[i,:] is the vector for word i\n",
    "        :param x: x[i, :] are the indices of the words in the i-th sentence\n",
    "        :param w: w[i, :] are the weights for the words in the i-th sentence\n",
    "        :param params.rmpc: if >0, remove the projections of the sentence embeddings to their first principal component\n",
    "        :return: emb, emb[i, :] is the embedding for sentence i\n",
    "        \"\"\"\n",
    "        emb = self.get_weighted_average(We, x, w)\n",
    "        print emb.shape\n",
    "        print \"Computed Embeddings\"\n",
    "        if self.args.pca:\n",
    "            emb = self.remove_pc(emb,self.args.n_components)\n",
    "        return emb\n",
    "\n",
    "    def getSIFEmbedding(self, sent):\n",
    "        corpus = [sent]\n",
    "        #concept_filenames = self.args.concept_filenames.split(\",\")\n",
    "        #clean_pages_space = map(lambda x: \" \" + \"  \".join(x.split()),\n",
    "        #                        corpus)  ##this line is **important** introduce double spaces\n",
    "        #tagged_corpus = phrase_tagger.concept_tagging(clean_pages_space, concept_filenames, \"t\")\n",
    "        #tagged_corpus.load_concepts()\n",
    "        #corpus_result = tagged_corpus.tag_corpus()\n",
    "        result = corpus[0]\n",
    "        words = result.split()\n",
    "        indices_list = filter(lambda x: x is not None, map(self.vocab.get, words))\n",
    "        #get average\n",
    "\n",
    "        count = len(indices_list)\n",
    "        sent_embedding = np.zeros((300),dtype=\"float32\")\n",
    "        for idx in indices_list:\n",
    "            sent_embedding = sent_embedding + self.word_embeddings[idx,:] * self.weights[0,idx]\n",
    "        if count > 0:\n",
    "            sent_embedding/=count\n",
    "        else:\n",
    "            print \"Empty Sentence\"\n",
    "\n",
    "        return sent_embedding\n",
    "\n",
    "#     def getRanking(self, sent,n=10):\n",
    "#         sent = sent.lower()\n",
    "#         sent = cleanContentPage(sent)\n",
    "#         sent_embedding = normalize(self.getSIFEmbedding(sent))\n",
    "#         print sent_embedding.shape\n",
    "#         print self.concepts_embeddings.shape\n",
    "#         conceptDistances = np.dot(self.concepts_embeddings, sent_embedding.T)\n",
    "#         conceptDistances = conceptDistances.reshape(conceptDistances.shape[0])\n",
    "#         rankedIndices = conceptDistances.argsort()[-n:][::-1]\n",
    "#         print rankedIndices\n",
    "#         for idx in rankedIndices:\n",
    "#             print self.concepts[idx]\n",
    "\n",
    "\n",
    "\n",
    "args = parse_args()\n",
    "print pprint.pformat(args.__dict__)\n",
    "if args.build_vocab:\n",
    "    dataset = Dataset(args.sentence_file, args.embeddings_file)\n",
    "    vocab = dataset.generate_vocab()\n",
    "\n",
    "    #write to files\n",
    "    write_vocab(vocab, args.vocab_file)\n",
    "    vocab = load_vocab(args.vocab_file)\n",
    "    export_trimmed_w2v_vectors(vocab, args.embeddings_file, args.trimmed_embedding_file)\n",
    "\n",
    "args.vocab = load_vocab(args.vocab_file)\n",
    "print len(args.vocab)\n",
    "args.word_embeddings = get_trimmed_w2v_vectors(args.trimmed_embedding_file)\n",
    "print args.word_embeddings.shape\n",
    "\n",
    "sif_model = SIF_Model(args)\n",
    "# sif_model.printShapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
